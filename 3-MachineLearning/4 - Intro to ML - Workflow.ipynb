{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Intro to ML\n",
    "## Workflow: Pipelines and Feature Unions\n",
    "---\n",
    "\n",
    "En la última sesión comentamos qué métricas nos permiten discriminar los modelos y cómo calcularlas con especial enfasis en métricas de clasificación.\n",
    "\n",
    "En la sesión de hoy, haremos una mención especial a cómo tratar de forma programática las distintas fases de un trabajo o proyecto de modelización: transformación de los datos, afinado de hiper-parametros y validación.\n",
    "\n",
    "\n",
    "<div class=\"panel panel-success\">\n",
    "    <div class='panel-heading'>\n",
    "    <h4>Empecemos</h4>\n",
    "    </div>\n",
    "    <div class='panel-body'>\n",
    "    <ol type=\"A\">\n",
    "    <li>Pipelines</li>\n",
    "    <li>Feature Unions</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pipelines\n",
    "---\n",
    "\n",
    "Al encarar un proyecto de modelizado de datos, antes de poder utilizar un motor de predicción o clasificación, tendremos que *jugar* con los datos con dos objetivos:\n",
    "\n",
    "1. Modificar/seleccionar las variables predictoras con tal de obtener las mejores predicciones\n",
    "1. Alimentar el algoritmo de ML con un formato de datos que sea capaz de utilizar\n",
    "\n",
    "Dicho de otra forma, los datos pasarán por una serie transformaciones y, por último, por un paso de modelizado. Cada uno de los pasos de la secuencia puede ser definido como un **componente** y el flujo de transformación como una serie de **componentes** encadenados.\n",
    "\n",
    "---\n",
    "\n",
    "<img src='img/pipeline.png'>\n",
    "\n",
    "---\n",
    "\n",
    "Las `pipelines` permiten encadenar los distintos componentes de forma secuencial de forma que la salida de un componente sea la entrada del siguiente.\n",
    "\n",
    "Veamos un pequeño ejemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparamos los datos con unos cuantos nan's..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "### adding some nan's\n",
    "mask = np.random.choice([0,1], p=[0.99, 0.01], size=X.shape).astype(np.bool)\n",
    "X[mask] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo de Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solo transformaciones\n",
    "\n",
    "Podemos usar el pipeline como un agregador de transformaciones que tendra un metodo,\n",
    ".fit() y otro .transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "## Dividimos los datos en test y train\n",
    "X_train, X_test, y_train, y_test = \n",
    "\n",
    "## Definimos los pasos del pipeline como tuplas (name, Transformer)\n",
    "imputation_step = ('imputer', Imputer())\n",
    "scaling_step = ...\n",
    "\n",
    "## Los ordenamos en una lista\n",
    "steps = [imputation_step, ...]\n",
    "\n",
    "## Finalmente llamamos al creador de pipeline\n",
    "pipe = Pipeline(steps)\n",
    "\n",
    "X_train_transformed = pipe.fit_transform(X_train)\n",
    "X_test_transformed = pipe.transform(X_test)\n",
    "\n",
    "print('X_train: \\n')\n",
    "print('Mean before pipeline: ', X_train.mean(axis=0))\n",
    "print('Mean after pipeline: ', X_train_transformed.mean(axis=0))\n",
    "\n",
    "print('\\n X_test: \\n')\n",
    "print('Mean before pipeline: ', X_test.mean(axis=0))\n",
    "print('Mean after pipeline: ', X_test_transformed.mean(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search sobre parametros de los transformadores\n",
    "\n",
    "Una de las ventajas de usar un pipeline, es que podemos definir un gridsearch donde los parametros de busqueda incluyan aquellos de los transformadores. Vamos a ver si es mejor usar la media o la mediana..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import ...\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "# importa un clasificador de Random Forest\n",
    "from sklearn.ensemble import ...\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## Dividimos los datos en test y train\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1234, shuffle=True)\n",
    "\n",
    "## Definimos los pasos del pipeline como tuplas (name, Transformer)\n",
    "imputation_step = ...\n",
    "scaling_step = ...\n",
    "model_step = ...\n",
    "\n",
    "## Los ordenamos en una lista\n",
    "steps = [...]\n",
    "\n",
    "## Finalmente llamamos al creador de pipeline\n",
    "pipe = Pipeline(steps)\n",
    "\n",
    "\n",
    "## Definimos el espacio de busqueda para el gridsearch.\n",
    "## Podemos acceder a los distintos hiperparametros de los Transformadores como nombre__hiperparametro\n",
    "\n",
    "param_grid = {'imputer__strategy': ['mean', 'median'],\n",
    "              'clf__max_depth': [3, 5, 10],\n",
    "              'clf__min_samples_leaf': [2, 5, 10]}\n",
    "\n",
    "## usamos el pipe como estimador en el gridsearch\n",
    "clf_gs = GridSearchCV(pipe, cv=3, n_jobs=-1, param_grid=param_grid)\n",
    "\n",
    "clf_gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best score: ', clf_gs.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder hacer predicciones, simplemente debemos llamar al clf_gs con el metodo predict.\n",
    "El pipeline se encarga de hacer las transformaciones necesarias antes de pasarle los datos al predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Haz unas predicciones y utiliza el classification report para ver como ha ido\n",
    "from sklearn.metrics import ...\n",
    "\n",
    "y_test_pred = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disgresión\n",
    "#### Feature engineering usando Principal Component Analysis\n",
    "\n",
    "La explicación de como funciona el analisis de componentes principales, o PCA por sus siglas en inglés, está fuera de los objetivos de esta sesión. Sin embargo, al ser una herramienta muy útil a la hora de trabajar con datos multi-dimensionales, merece la pena una mención de como usarlo a nivel práctico.\n",
    "\n",
    "Realizar PCA sobre un set de datos multidimensional nos permitirá:\n",
    "\n",
    "1. Visualizar una proyección de los datos en 2-3 dimensiones\n",
    "1. Reducir el número de predictores: seleccionaremos un número límitado de componentes de forma que expliquemos la mayor parte de la varianza de los datos\n",
    "\n",
    "Por ejemplo..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.decomposition import ...\n",
    "from sklearn.preprocessing import ...\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "pca = ...\n",
    "scaler = ...\n",
    "\n",
    "X_scaled = ...\n",
    "X_pca = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## accede a los atributos/metodos del objeto pca\n",
    "\n",
    "print(pca.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f, ax = plt.subplots(ncols=2, figsize=(12, 5))\n",
    "\n",
    "ax[0].scatter(X_pca[y==0][:,0], X_pca[y==0][:, 1], c='green', alpha=0.5, label='Not Cancer')\n",
    "ax[0].scatter(X_pca[y==1][:,0], X_pca[y==1][:, 1], c='firebrick', alpha=0.5, label='Cancer')\n",
    "ax[0].legend(loc='upper left')\n",
    "ax[0].set_title('2D Projection')\n",
    "\n",
    "\n",
    "### Varianza explicada\n",
    "ax[1].plot(pca.explained_variance_ratio_.cumsum())\n",
    "ax[1].set_xlabel('Num of PCA components')\n",
    "ax[1].set_title('Explained Variance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio: Traduce el siguiente script a un pipeline y evalualo en el dataset de testeo.\n",
    "\n",
    "Nota: En vez de seleccionar manualmente el umbral de varianza explicada para el PCA, introduce el número de componentes en el gridsearch del pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "data = load_wine()\n",
    "\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['y'] = data.target\n",
    "\n",
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('y', axis=1), df.y,\n",
    "                                                    test_size = 0.33, random_state=123)\n",
    "###########################################\n",
    "## Transformation 1: Polynomial Features ##\n",
    "###########################################\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "#instantiate polynomial features\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True)\n",
    "\n",
    "# fit scaler to train data\n",
    "poly.fit(X_train)\n",
    "\n",
    "# scale X_train\n",
    "X_train_poly= poly.transform(X_train)\n",
    "\n",
    "#######################################\n",
    "## Transformation 2: feature scaling ##\n",
    "#######################################\n",
    "\n",
    "## It is important to only to fit the transformer on the train dataset!\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# instantiate scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# fit scaler to train data\n",
    "scaler.fit(X_train_poly)\n",
    "\n",
    "# scale X_train\n",
    "X_train_poly_scaled= scaler.transform(X_train_poly)\n",
    "\n",
    "###########################\n",
    "## Transformation 3: PCA ##\n",
    "###########################\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# instantiate pca\n",
    "pca = PCA()\n",
    "\n",
    "# fit PCA to scaled train data\n",
    "pca.fit(X_train_poly_scaled)\n",
    "\n",
    "# scale X_train\n",
    "X_train_poly_scaled_pca = pca.transform(X_train_poly_scaled)\n",
    "\n",
    "#########################################\n",
    "## Transformation 4: Feature Selection ##\n",
    "#########################################\n",
    "\n",
    "# Num of components needed to reach 95%\n",
    "num_comp_95 = pca.explained_variance_ratio_[pca.explained_variance_ratio_.cumsum() < 0.95].shape[0]\n",
    "\n",
    "# instantiate pca with number of components desired\n",
    "pca = PCA(num_comp_95)\n",
    "\n",
    "# refit PCA to scaled train data\n",
    "pca.fit(X_train_poly_scaled)\n",
    "\n",
    "# scale X_train\n",
    "X_train_final = pca.transform(X_train_poly_scaled)\n",
    "\n",
    "##################\n",
    "## Modelization ##\n",
    "##################\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "## Define stratified kfold\n",
    "skfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=1234)\n",
    "\n",
    "## Instantiate Random Forest\n",
    "clf_rf = RandomForestClassifier()\n",
    "\n",
    "## set up a parameter grid for the gridsearch\n",
    "params_clf_rf = {'max_depth': [2, 3, 4],\n",
    "                 'min_samples_leaf': [8, 9, 10, 11, 12],\n",
    "                 'min_samples_split': [3, 4, 5, 6, 7]}\n",
    "\n",
    "## Instantiate the gridsearch\n",
    "clf_gs = GridSearchCV(estimator=clf_rf, cv=skfold, param_grid=params_clf_rf, verbose=1, n_jobs=-1)\n",
    "\n",
    "## Fit the gridsearch\n",
    "clf_gs.fit(X_train_final, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "## Transformation 1: Polynomial Features ##\n",
    "###########################################\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "#instantiate polynomial features\n",
    "step_poly = ...\n",
    "\n",
    "#######################################\n",
    "## Transformation 2: feature scaling ##\n",
    "#######################################\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "step_scaler = ...\n",
    "\n",
    "###########################\n",
    "## Transformation 3: PCA ##\n",
    "###########################\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "step_pca = ...\n",
    "\n",
    "##################\n",
    "## Modelization ##\n",
    "##################\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model_step = ...\n",
    "\n",
    "##########################\n",
    "## Make pipe            ##\n",
    "##########################\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe_steps = [...]\n",
    "\n",
    "pipe = Pipeline(...)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "## Define stratified kfold\n",
    "skfold = ...\n",
    "\n",
    "params_pipe = {'poly__degree': [2, 3],\n",
    "               'pca__n_components': [9, 10, 11],\n",
    "               'clf__max_depth': [3, 4],\n",
    "               'clf__min_samples_leaf': [8, 9, 10],\n",
    "               'clf__min_samples_split': [5, 6, 7]}\n",
    "\n",
    "## Instantiate the gridsearch\n",
    "clf_gs = GridSearchCV(...)\n",
    "\n",
    "## Fit the gridsearch\n",
    "clf_gs.fit(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf_gs.best_estimator_.named_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_pred = clf_gs.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature unions\n",
    "\n",
    "Hasta ahora, solo hemos trabajado con un tipo de variables: númericas. Sin embargo, si tuvieramos variables categóricas, deberíamos tratarlas de forma distinta. Por ejemplo, haciendo un OneHotEncoding.\n",
    "\n",
    "Las feature unions nos permiten trabajar con transformaciones según el tipo de variable, para despues unirlas antes de pasarlas al modelo de ML.\n",
    "\n",
    "<img src='img/pipeline+featunion.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data', header=None)\n",
    "#data.columns = ['sex', 'length', 'diameter', 'height', 'whole_weight', 'shucked_weight', 'viscera_weight', 'shell_weight', 'y_rings']\n",
    "\n",
    "data = pd.read_csv('data/abalone.csv',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['sex'] = data['sex'].map({'F':0, 'I':1, 'M':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "### aux functions\n",
    "\n",
    "class SelectColumns(TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "    def fit(self, X, *y):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[[self.columns]].values\n",
    "    \n",
    "class DropColumns(TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "    def fit(self, X, *y):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X.drop([self.columns], axis=1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** transformation to one-hot-encode 'sex' feature**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "select_col_step = ('select', SelectColumns('sex'))\n",
    "one_hot_step = ('sex_one_hot', OneHotEncoder())\n",
    "cat_pipe_steps = [select_col_step, one_hot_step]\n",
    "cat_pipe = Pipeline(cat_pipe_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** transformation of the numeric values by MinMaxScaler **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "drop_column_step = ('drop_column', DropColumns('sex'))\n",
    "poly_step = ('poly', PolynomialFeatures(4,  interaction_only=True))\n",
    "scaler_step = ('scaler', MinMaxScaler())\n",
    "num_pipe_steps = [drop_column_step, poly_step, scaler_step]\n",
    "num_pipe = Pipeline(num_pipe_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Join the two pipes using feature union **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "transformer_list = [('num_pipe', num_pipe),\n",
    "                    ('cat_pipe', cat_pipe)]\n",
    "\n",
    "full_pipe = FeatureUnion(transformer_list=transformer_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data.drop('y_rings', axis=1), data.y_rings, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reg = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_trans = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.fit(..., ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_pred_pipe = reg.predict(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ...\n",
    "\n",
    "print(mean_absolute_error(y_test, y_test_pred_pipe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bins = y_test.unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_test, alpha=0.5, bins=bins, label='Real')\n",
    "plt.hist(y_test_pred_pipe, alpha=0.5, bins=bins, label='Predicted')\n",
    "plt.xlabel('rings')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, y_test_pred_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
